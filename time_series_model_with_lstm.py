# -*- coding: utf-8 -*-
"""Time_Series_Model_With_LSTM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zKJfcJODWR6qZWqPOlSZDAvltL0KzcKO
"""

import numpy as np
import pandas as pd
from keras.layers import Dense, LSTM
import matplotlib.pyplot as plt
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

data = pd.read_csv('COCO COLA.csv')
data.head()

data.info()

data.isnull().sum()

date = data['Date'].values
price = data['Close'].values

plt.figure(figsize=(20,8))
plt.plot(date, price)
plt.title('Coca Cola Stock Price (1962 - 2021)')
plt.xlabel('Date')
plt.ylabel('Price')
plt.show()

scaler = MinMaxScaler()
price_scaled = scaler.fit_transform(price.reshape(-1, 1)).flatten()

threshold = (price.max() - price.min()) * 10 / 100
print('Threshold MAE:', threshold)

x_train, x_test, y_train, y_test = train_test_split(price, date, test_size = 0.2, shuffle=False)

def windowed_dataset(series, window_size, batch_size, shuffle_buffer):
  series = tf.expand_dims(series, axis=-1)
  data_series = tf.data.Dataset.from_tensor_slices(series)
  data_series = data_series.window(window_size + 1, shift=1, drop_remainder=True)
  data_series = data_series.flat_map(lambda w: w.batch(window_size + 1))
  data_series = data_series.shuffle(shuffle_buffer)
  data_series = data_series.map(lambda w: (w[:-1], w[-1:]))
  return data_series.batch(batch_size).prefetch(1)

window_size = 128
batch_size = 64
shuffle_buffer = 1000

data_training = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer)
data_testing = windowed_dataset(x_test, window_size, batch_size, shuffle_buffer)

model = tf.keras.models.Sequential([
  tf.keras.layers.LSTM(128, return_sequences=True),
  tf.keras.layers.LSTM(128),
  tf.keras.layers.Dense(50, activation="relu"),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(20, activation="relu"),
  tf.keras.layers.Dropout(0.5),
  tf.keras.layers.Dense(1),
])

optimizers = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(loss='mse',
              optimizer=optimizers,
              metrics=["mae"])

def scheduler(epoch, lr):
    if epoch < 20:
        return lr
    else:
        return lr * tf.math.exp(-0.1)

lr_scheduler = tf.keras.callbacks.LearningRateScheduler(scheduler)

class EarlyStoppingByMAE(tf.keras.callbacks.Callback):
    def on_epoch_end(self, epoch, logs=None):
        if logs.get('mae') < threshold and logs.get('val_mae') < threshold:
            print("MAE and Val_MAE below threshold, stopping training")
            self.model.stop_training = True

callbacks = [EarlyStoppingByMAE(), lr_scheduler]

model_history = model.fit(
    data_training,
    epochs=200,
    validation_data=data_testing,
    callbacks=callbacks
)